"""Vulnerability profiling: identify scenarios and parameters that cause most degradation."""

import numpy as np
from typing import Dict, List, Tuple
from .base import StressSummary
from .robustness import Robustness


class VulnerabilityProfile:
    """
    Analyze a stress test summary to find the most damaging scenarios and
    the most sensitive parameters.
    """
    
    def __init__(self, summary: StressSummary):
        self.summary = summary
        # Pre‑compute per‑scenario degradation (average across metrics)
        self._scenario_scores = self._compute_scenario_scores()
    
    def _compute_scenario_scores(self) -> Dict[str, float]:
        """Compute a degradation score (0 = worst, 1 = best) for each scenario."""
        # Reuse Robustness logic with tolerance=1.0 to get a raw degradation measure
        scores = {}
        for res in self.summary.results:
            if not res.success:
                scores[res.scenario_name] = 0.0
                continue
            # Simple average of normalized degradation
            degs = []
            for metric, base in self.summary.baseline_outputs.items():
                val = res.outputs.get(metric, base)
                if abs(base) < 1e-10:
                    deg = 1.0 if abs(val) > 1e-10 else 0.0
                else:
                    deg = min(1.0, abs(val - base) / abs(base))
                degs.append(deg)
            avg_deg = np.mean(degs)
            scores[res.scenario_name] = 1.0 - avg_deg  # convert to score (higher better)
        return scores
    
    def worst_scenarios(self, n: int = 5) -> List[Tuple[str, float]]:
        """Return the n scenarios with lowest scores (most damaging)."""
        sorted_items = sorted(self._scenario_scores.items(), key=lambda x: x[1])
        return sorted_items[:n]
    
    def best_scenarios(self, n: int = 5) -> List[Tuple[str, float]]:
        """Return the n scenarios with highest scores (least damaging)."""
        sorted_items = sorted(self._scenario_scores.items(), key=lambda x: -x[1])
        return sorted_items[:n]
    
    def parameter_sensitivity(self) -> Dict[str, float]:
        """
        Estimate which parameters (modification targets) cause most degradation.
        For each parameter, average the scores of scenarios that modify it.
        Returns dict mapping param_name -> average score (lower = more sensitive).
        """
        # This requires linking back to scenario objects, which we don't have in the summary.
        # We'll implement a simplified version that uses scenario names (if they contain the param name).
        param_scores = {}
        param_counts = {}
        for res in self.summary.results:
            # Guess parameter from scenario name (crude)
            name = res.scenario_name.lower()
            for param in ['gravity', 'friction', 'force',
                         'w03', 'w04', 'w13', 'w14', 'w23', 'w24',
                         'w33', 'w34', 'w43', 'w44']:
                if param in name:
                    score = self._scenario_scores.get(res.scenario_name, 0.5)
                    if param not in param_scores:
                        param_scores[param] = []
                    param_scores[param].append(score)
        # Average
        return {p: float(np.mean(scores)) for p, scores in param_scores.items()}
    
    def report(self) -> Dict:
        """Generate a comprehensive vulnerability report."""
        return {
            'worst_scenarios': self.worst_scenarios(5),
            'best_scenarios': self.best_scenarios(5),
            'overall_robustness': Robustness(self.summary).score(),
            'per_metric_robustness': Robustness(self.summary).per_metric_scores(),
            'parameter_sensitivity': self.parameter_sensitivity()
        }
